{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b09dfce9-0c87-446f-ad2c-eac213cba849",
   "metadata": {},
   "source": [
    "**SOURCES**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d806215-4108-4561-a6da-313e6b840277",
   "metadata": {},
   "source": [
    "https://spark.apache.org/docs/latest/api/python/index.html\n",
    "\n",
    "https://www.kaggle.com/code/nezarabdilahprakasa/sentiment-analysis-using-pyspark-big-data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17c8f1b-f9f0-414a-95ca-86df8b0e6a2a",
   "metadata": {},
   "source": [
    "**MODEL PREPARATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c443c131-4878-423b-b97f-7182c0464c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer, StopWordsRemover\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6c9a90d-f44e-4b61-b915-7bc4fd169a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data check\n",
    "df = pd.read_csv(\"spark_set.csv\", encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33cba581-18d7-4444-b269-5a66fbaf6829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Lemmas_Text', 'vader_sentiment'], dtype='object')\n",
      "2112619\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)\n",
    "print(df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e25885d3-0277-4b9e-ac49-929f583668b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lemmas_Text</th>\n",
       "      <th>vader_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>virginamerica dhepburn said</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>virginamerica plus youve added commercial expe...</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>virginamerica didnt today mean need trip</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>virginamerica really aggressive blast obnoxiou...</td>\n",
       "      <td>-0.3306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>virginamerica really big bad thing</td>\n",
       "      <td>-0.5829</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Lemmas_Text  vader_sentiment\n",
       "0                        virginamerica dhepburn said           0.0000\n",
       "1  virginamerica plus youve added commercial expe...           0.0000\n",
       "2           virginamerica didnt today mean need trip           0.0000\n",
       "3  virginamerica really aggressive blast obnoxiou...          -0.3306\n",
       "4                 virginamerica really big bad thing          -0.5829"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc2971c1-9455-4a1f-9530-58fc3e454d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/13 12:56:37 WARN Utils: Your hostname, halloa93 resolves to a loopback address: 127.0.1.1; using 192.168.55.114 instead (on interface wlp2s0)\n",
      "24/06/13 12:56:37 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/13 12:56:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#session create\n",
    "spark = SparkSession.builder.master('local').appName(\"pyspark_model_training\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c32aebc2-b293-4650-bd56-d5e5824aeeda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------+---------------+\n",
      "|Lemmas_Text                                               |vader_sentiment|\n",
      "+----------------------------------------------------------+---------------+\n",
      "|virginamerica dhepburn said                               |0.0            |\n",
      "|virginamerica plus youve added commercial experience tacky|0.0            |\n",
      "|virginamerica didnt today mean need trip                  |0.0            |\n",
      "+----------------------------------------------------------+---------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#dataframe load\n",
    "df = spark.read.csv(\"spark_set.csv\", header = True, inferSchema = True)\n",
    "df.show(truncate = False, n = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1954c3a9-ab02-425c-a86f-2efa02ff1f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------------------+\n",
      "|vader_sentiment|vader_sentiment_transformed|\n",
      "+---------------+---------------------------+\n",
      "|0.0            |0                          |\n",
      "|0.0            |0                          |\n",
      "|0.0            |0                          |\n",
      "|-0.3306        |0                          |\n",
      "|-0.5829        |0                          |\n",
      "|0.0963         |0                          |\n",
      "|0.4019         |0                          |\n",
      "|0.1458         |0                          |\n",
      "|0.0            |0                          |\n",
      "|0.7717         |1                          |\n",
      "|-0.8555        |0                          |\n",
      "|0.7269         |1                          |\n",
      "|0.6249         |1                          |\n",
      "|0.1531         |0                          |\n",
      "|0.4404         |0                          |\n",
      "|-0.296         |0                          |\n",
      "|0.7579         |1                          |\n",
      "|0.4019         |0                          |\n",
      "|0.0            |0                          |\n",
      "|0.7146         |1                          |\n",
      "|0.0            |0                          |\n",
      "|0.6369         |1                          |\n",
      "|0.8658         |1                          |\n",
      "|-0.296         |0                          |\n",
      "|0.6597         |1                          |\n",
      "|0.0            |0                          |\n",
      "|-0.5423        |0                          |\n",
      "|0.2037         |0                          |\n",
      "|0.5859         |1                          |\n",
      "|-0.0516        |0                          |\n",
      "|-0.1027        |0                          |\n",
      "|0.0            |0                          |\n",
      "|0.4588         |0                          |\n",
      "|0.0            |0                          |\n",
      "|0.6249         |1                          |\n",
      "|0.4215         |0                          |\n",
      "|0.8481         |1                          |\n",
      "|0.8126         |1                          |\n",
      "|0.0            |0                          |\n",
      "|0.4019         |0                          |\n",
      "|0.0            |0                          |\n",
      "|0.5719         |1                          |\n",
      "|0.7783         |1                          |\n",
      "|0.0            |0                          |\n",
      "|0.0            |0                          |\n",
      "|0.4404         |0                          |\n",
      "|0.25           |0                          |\n",
      "|0.5859         |1                          |\n",
      "|0.3612         |0                          |\n",
      "|0.4404         |0                          |\n",
      "+---------------+---------------------------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#transforming vader_sentimetn form Float to Int\n",
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "df = df.withColumn(\"vader_sentiment_transformed\",\n",
    "                   when(col(\"vader_sentiment\") >= 0.5, 1).\n",
    "                   when(col(\"vader_sentiment\") <= -0.5, 0).\n",
    "                   otherwise(0))\n",
    "\n",
    "\n",
    "df.select(\"vader_sentiment\", \"vader_sentiment_transformed\").show(truncate=False, n=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d588b0ed-2c65-4d33-88b6-3326216bb9ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------+-----+\n",
      "|Lemmas_Text                                                                                 |label|\n",
      "+--------------------------------------------------------------------------------------------+-----+\n",
      "|virginamerica dhepburn said                                                                 |0    |\n",
      "|virginamerica plus youve added commercial experience tacky                                  |0    |\n",
      "|virginamerica didnt today mean need trip                                                    |0    |\n",
      "|virginamerica really aggressive blast obnoxious entertainment guest face amp little recourse|0    |\n",
      "|virginamerica really big bad thing                                                          |0    |\n",
      "+--------------------------------------------------------------------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_mod = df.select(\"Lemmas_Text\", col(\"vader_sentiment_transformed\").cast(\"Int\").alias(\"label\"))\n",
    "df_mod.show(truncate = False, n = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6580f4d2-5ba3-4bb5-8dbf-325d172ed027",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train rows =  1900950\n",
      "Test rows =  211669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#seting parameters for data split train/test\n",
    "df_split = df_mod.randomSplit([0.9, 0.1])\n",
    "train = df_split[0]\n",
    "\n",
    "test = df_split[1].withColumnRenamed(\"label\", \"true_label\")\n",
    "train_rows = train.count()\n",
    "test_rows = test.count()\n",
    "\n",
    "print(\"Train rows = \", train_rows)\n",
    "print(\"Test rows = \", test_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cec63758-dd26-4935-ade8-2767d9e08c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Lemmas_Text: string (nullable = true)\n",
      " |-- label: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "626a24bd-69ec-4df6-bc0f-19dee9affdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing NULL \n",
    "train = train.na.drop(subset = [\"Lemmas_Text\"])\n",
    "train = train.na.drop(subset = [\"label\"])\n",
    "test = test.na.drop(subset = [\"Lemmas_Text\"])\n",
    "test = test.na.drop(subset = [\"true_label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ccf25e5-1909-4197-a713-43d83e6f2c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+\n",
      "|         Lemmas_Text|label|               Words|\n",
      "+--------------------+-----+--------------------+\n",
      "|aa definitely wor...|    0|[aa, definitely, ...|\n",
      "|aa manually enter...|    0|[aa, manually, en...|\n",
      "|      aaa better kkk|    0|  [aaa, better, kkk]|\n",
      "|aaa conecte desde...|    0|[aaa, conecte, de...|\n",
      "|aaa handling dmv ...|    0|[aaa, handling, d...|\n",
      "+--------------------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#data tokenization\n",
    "tokenizer = Tokenizer(inputCol = \"Lemmas_Text\", outputCol = \"Words\")\n",
    "tokenizedTrain = tokenizer.transform(train)\n",
    "tokenizedTrain.show(truncate = True, n = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b349588-9d54-456f-bc7a-d6a4e463a98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+\n",
      "|         Lemmas_Text|label|               Words|     MeaningfulWords|\n",
      "+--------------------+-----+--------------------+--------------------+\n",
      "|aa definitely wor...|    0|[aa, definitely, ...|[aa, definitely, ...|\n",
      "|aa manually enter...|    0|[aa, manually, en...|[aa, manually, en...|\n",
      "|      aaa better kkk|    0|  [aaa, better, kkk]|  [aaa, better, kkk]|\n",
      "|aaa conecte desde...|    0|[aaa, conecte, de...|[aaa, conecte, de...|\n",
      "|aaa handling dmv ...|    0|[aaa, handling, d...|[aaa, handling, d...|\n",
      "+--------------------+-----+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#removing stop words\n",
    "swr = StopWordsRemover(inputCol = tokenizer.getOutputCol(), outputCol = \"MeaningfulWords\")\n",
    "SwRemovedTrain = swr.transform(tokenizedTrain)\n",
    "SwRemovedTrain.show(truncate = True, n = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1ec152d-6d54-4c7a-a72d-b3e6d1794014",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|label|     MeaningfulWords|            features|\n",
      "+-----+--------------------+--------------------+\n",
      "|    0|[aa, definitely, ...|(262144,[6346,714...|\n",
      "|    0|[aa, manually, en...|(262144,[5078,978...|\n",
      "|    0|  [aaa, better, kkk]|(262144,[115611,1...|\n",
      "|    0|[aaa, conecte, de...|(262144,[50572,13...|\n",
      "|    0|[aaa, handling, d...|(262144,[70769,74...|\n",
      "+-----+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#hashing data\n",
    "hashTF = HashingTF(inputCol = swr.getOutputCol(), outputCol = \"features\")\n",
    "numericTrain = hashTF.transform(SwRemovedTrain).select('label', 'MeaningfulWords', 'features')\n",
    "numericTrain.show(truncate = True, n = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd37b35-ec1b-477e-99b4-f94703a8a9f3",
   "metadata": {},
   "source": [
    "**LOGISTIC REGRESSION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b66f9d96-6d9d-45ea-a396-7a755abe51c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Done\n"
     ]
    }
   ],
   "source": [
    "#logistic regression model training\n",
    "lr = LogisticRegression(labelCol = \"label\", featuresCol = \"features\", maxIter = 10, regParam = 0.01)\n",
    "model_lr = lr.fit(numericTrain)\n",
    "print(\"Training Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9d78fac-dd31-489e-b20d-9c80fa539ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 26:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------------------+--------------------+--------------------+\n",
      "|         Lemmas_Text|true_label|               Words|     MeaningfulWords|            features|\n",
      "+--------------------+----------+--------------------+--------------------+--------------------+\n",
      "|aaa motherfucking...|         0|[aaa, motherfucki...|[aaa, motherfucki...|(262144,[98592,17...|\n",
      "|aaaa house mess n...|         0|[aaaa, house, mes...|[aaaa, house, mes...|(262144,[41660,89...|\n",
      "|aaaaa omg sophiel...|         1|[aaaaa, omg, soph...|[aaaaa, omg, soph...|(262144,[3917,561...|\n",
      "|   aaaaaaaaaaa mcfly|         0|[aaaaaaaaaaa, mcfly]|[aaaaaaaaaaa, mcfly]|(262144,[51537,12...|\n",
      "|aaaaaaaaaaaaaa do...|         0|[aaaaaaaaaaaaaa, ...|[aaaaaaaaaaaaaa, ...|(262144,[113432,1...|\n",
      "+--------------------+----------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#logistic regression model testing\n",
    "tokenizedTest = tokenizer.transform(test)\n",
    "SwRemovedTest = swr.transform(tokenizedTest)\n",
    "numericTest = hashTF.transform(SwRemovedTest)\n",
    "numericTest.show(truncate = True, n = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "90096488-0f83-4af2-a178-8690078f6256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Lemmas_Text: string (nullable = true)\n",
      " |-- true_label: integer (nullable = false)\n",
      " |-- Words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- MeaningfulWords: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#transforming\n",
    "raw_prediction_lr = model_lr.transform(numericTest)\n",
    "raw_prediction_lr.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c8e55a9c-8880-47e3-92e7-f1389ba3e386",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/13 13:00:25 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "[Stage 27:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+----------+\n",
      "|     MeaningfulWords|prediction|true_label|\n",
      "+--------------------+----------+----------+\n",
      "|[aaa, motherfucki...|       0.0|         0|\n",
      "|[aaaa, house, mes...|       0.0|         0|\n",
      "|[aaaaa, omg, soph...|       1.0|         1|\n",
      "|[aaaaaaaaaaa, mcfly]|       0.0|         0|\n",
      "|[aaaaaaaaaaaaaa, ...|       0.0|         0|\n",
      "+--------------------+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "lr_prediction = raw_prediction_lr.select(\"MeaningfulWords\", \"prediction\", \"true_label\")\n",
    "lr_prediction.show(truncate = True, n = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8198a658-7a42-4a5d-9e7b-5c8374e344a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/13 13:00:32 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "[Stage 31:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score achived is:  93.99581421937081 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "total_lr_true = lr_prediction.filter(lr_prediction['prediction'] == lr_prediction['true_label']).count()\n",
    "alldata = lr_prediction.count()\n",
    "accuracy = total_lr_true/alldata\n",
    "print(\"Score achived is: \", accuracy*100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8ddc3291-e0c4-4f46-8154-052e0fda57c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/13 13:01:03 WARN TaskSetManager: Stage 35 contains a task of very large size (2097 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#model save for future automatization\n",
    "model_lr.save(\"lr_model_reference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f27725-e4f8-4c4b-88da-20ada00a2f13",
   "metadata": {},
   "source": [
    "**NAIVE BAYES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "219f113b-a0ae-4145-998b-d434fb9aaf06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 38:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "nb = NaiveBayes(labelCol = \"label\", featuresCol = \"features\", smoothing = 1.0, modelType = \"multinomial\")\n",
    "model_nb = nb.fit(numericTrain)\n",
    "print(\"Training Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "883cfe7a-9a39-4a15-acd3-de5ba7efa599",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 41:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------------------+--------------------+--------------------+\n",
      "|         Lemmas_Text|true_label|               Words|     MeaningfulWords|            features|\n",
      "+--------------------+----------+--------------------+--------------------+--------------------+\n",
      "|aaa motherfucking...|         0|[aaa, motherfucki...|[aaa, motherfucki...|(262144,[98592,17...|\n",
      "|aaaa house mess n...|         0|[aaaa, house, mes...|[aaaa, house, mes...|(262144,[41660,89...|\n",
      "|aaaaa omg sophiel...|         1|[aaaaa, omg, soph...|[aaaaa, omg, soph...|(262144,[3917,561...|\n",
      "|   aaaaaaaaaaa mcfly|         0|[aaaaaaaaaaa, mcfly]|[aaaaaaaaaaa, mcfly]|(262144,[51537,12...|\n",
      "|aaaaaaaaaaaaaa do...|         0|[aaaaaaaaaaaaaa, ...|[aaaaaaaaaaaaaa, ...|(262144,[113432,1...|\n",
      "+--------------------+----------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "tokenizedTest = tokenizer.transform(test)\n",
    "SwRemovedTest = swr.transform(tokenizedTest)\n",
    "numericTest = hashTF.transform(SwRemovedTest)\n",
    "numericTest.show(truncate = True, n = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2ef6fab8-8005-4de8-b518-67ed8df6fd02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Lemmas_Text: string (nullable = true)\n",
      " |-- true_label: integer (nullable = false)\n",
      " |-- Words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- MeaningfulWords: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_prediction_nb = model_nb.transform(numericTest)\n",
    "raw_prediction_nb.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "14d7e295-fb05-4d15-b7c5-3dbb01fbd358",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/13 13:03:18 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "[Stage 42:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+----------+\n",
      "|     MeaningfulWords|prediction|true_label|\n",
      "+--------------------+----------+----------+\n",
      "|[aaa, motherfucki...|       0.0|         0|\n",
      "|[aaaa, house, mes...|       0.0|         0|\n",
      "|[aaaaa, omg, soph...|       0.0|         1|\n",
      "|[aaaaaaaaaaa, mcfly]|       0.0|         0|\n",
      "|[aaaaaaaaaaaaaa, ...|       1.0|         0|\n",
      "+--------------------+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "nb_prediction = raw_prediction_nb.select(\"MeaningfulWords\", \"prediction\", \"true_label\")\n",
    "nb_prediction.show(truncate = True, n = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cdd6b4da-d432-45f3-97de-a7701fdf651c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/13 13:03:26 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "[Stage 46:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score achived is:  89.23933121997081 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "total_nb_true = nb_prediction.filter(nb_prediction['prediction'] == nb_prediction['true_label']).count()\n",
    "alldata = nb_prediction.count()\n",
    "accuracy = total_nb_true/alldata\n",
    "print(\"Score achived is: \", accuracy*100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0b819852-8a92-4a12-bc81-0c08a0bb6db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/13 13:03:41 WARN TaskSetManager: Stage 50 contains a task of very large size (4188 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    }
   ],
   "source": [
    "model_nb.save(\"nb_model_reference\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
